{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "### Word focused\n",
    "If the vocabulary is what you are looking at, the next step is to remove the punctuation and the stopwords. \n",
    "\n",
    "With the following code, we are going to generate a list of words used in the `Body` column. It won't be extremely useful for the classification task but comparing vocabulary can be very revealing (given the right subset: spam vs. regular emails,etc.).\n",
    "\n",
    "Once we have a list of words, we can start the vocabulary analysis by coding a counter or using `FreqDist` from the `nltk` library for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv', index_col='Id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['Body']:\n",
    "    tokens = word_tokenize(text)\n",
    "    # converts to lower case\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    # removes the stopwords\n",
    "    words = [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = list(string.punctuation)\n",
    "stop_words += stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toklowstop(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # converts to lower case\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    # removes the stopwords\n",
    "    words = [word for word in tokens if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Body'].apply(lambda x: toklowstop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist(words).plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud\n",
    "Wordclouds are to text data what pie charts are to numerical data. They are at best confusing and at worst useless. But it can be generate a nice picture for your article header :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 2000,\n",
    "                      stopwords = stop_words)\n",
    "\n",
    "wordcloud.generate(\" \".join(words))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "This process is going to remove the ending of the words, shrinking them to their stem, their common denominator. For example, in the following list:\n",
    "- programmer\n",
    "- programmation\n",
    "- programmed\n",
    "- programming\n",
    "- program\n",
    "- programme\n",
    "\n",
    "The stem of these words is **program**. It is easy to imagine a stemmed document being harder to read (by a human at least) but at the same time, it is going to be easier to compare different documents. This \"normalization\" is going to help make a model more robust as the ambiguity is reduced.\n",
    "\n",
    "It is worth noting:\n",
    "- there are a lot of stemming algorithms, available in various languages. [nltk.stem API module](https://www.nltk.org/api/nltk.stem.html) will give you a list of stemming classes available with NLTK.\n",
    "- whilst stemming simplifies a document, it also creates \"new\" noise:  the stem for \"flies\" is \"fli\" for example.\n",
    "- whilst stemming simplifies a document, it will induce a loss of information. For example, in the \"program\" list above, programme is the British spelling and program, the American spelling. If the origin of the review is not an important information for your analysis: Great, stemming has made this simpler! However if the localisation is key to your problem, you might miss some nuances by stemming your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(document):\n",
    "    \"\"\"\n",
    "    Stemming words using \n",
    "    \"\"\"\n",
    "    doc_split = document.split(' ')\n",
    "    stemmed = ''\n",
    "    for word in doc_split:\n",
    "        stemmed += stemmer.stem(word) + ' '\n",
    "        \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit6d9251475e3c4a3a82c639eb6cfa00a6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
