{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "# from tensorflow.keras.layers import GRU\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import MaxPool1D\n",
    "# from tensorflow.keras.layers import SpatialDropout1D\n",
    "# from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/data_with_stem_lem.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('code', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Padded Train Test Split\n",
    "### X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 73_747 # From 03-WordAnalysis.ipynb\n",
    "# vocab_size = 290_479 # No restriction on vocab length\n",
    "token = Tokenizer(num_words=vocab_size,\n",
    "                  filters=\"\"\"!\"#$'%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\",\n",
    "                  lower=True\n",
    "                 )\n",
    "\n",
    "token.fit_on_texts(df['Body'].values)\n",
    "X = token.texts_to_sequences(df['Body'].values)\n",
    "len(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "# This number could be tweaked (look at histogram of word count per class)\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 150)\n",
      " \n",
      "[[   2    6  228 ...    0    0    0]\n",
      " [ 110  664    3 ...    0    0    0]\n",
      " [   2  113    3 ...    0    0    0]\n",
      " ...\n",
      " [1731  507    1 ...    0    0    0]\n",
      " [ 488  184   87 ...    0    0    0]\n",
      " [ 744 2490  186 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_X.shape, ' ', padded_X, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Y'].replace({'HQ':2,'LQ_EDIT':1,'LQ_CLOSE':0}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 150) (48000,)\n",
      "(12000, 150) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb_model = gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_pred = gnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.42      0.07      0.13     16037\n",
      "           1       0.65      0.23      0.34     16023\n",
      "           0       0.38      0.93      0.54     15940\n",
      "\n",
      "    accuracy                           0.41     48000\n",
      "   macro avg       0.48      0.41      0.34     48000\n",
      "weighted avg       0.48      0.41      0.34     48000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gaussian Naive Bayes Classification Report\", \" \",classification_report(Y_train, gnb_pred, labels= [2, 1, 0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\n",
      "[[80], [132], [103], [1184], [642], [64], [1], [125], [330], [1956], [5569], [10127], [781], [261], [1956], [1364], [397], [1975], [323], [16], [], [655], [323], [1], [1371], [3544], [415], [1494], [1], [119], [602, 467, 16], [16], [], [15665], [323], [1], [620], [181], [350, 2677], [718], [533], [323], [1651], [16], [323], [1], [620], [181], [7, 4791], [718], [2308], [2], [579], [14], [497], [866], [2522], [103], [523], [541], [595], [22290], [185], [2], [37], [32], [1], [533], [103], [361], [464], [847], [9717], [97], [551], [54], [579], [2672], [125], [1975], [4655], [323], [16], [2806], [323], [1042], [464], [923], [2], [579], [557], [3112], [125], [2672], [2514], [323], [39], [2672], [2522], [546], [320], [132], [377], [392], [542], [132], [1316], [103], [1651], [39], [1494], [6232], [1316], [2348], [9747], [39], [323], [1], [132], [15868], [614], [1956], [3918], [1368], [185], [323], [195], [2828], [334], [1160]]\n",
      "[[80]\n",
      " [ 0]\n",
      " [ 0]\n",
      " ...\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\n",
      "[[2], [1032], [70], [110], [18186], [533], [4829], [4689], [6459], [1], [726], [2856], [29], [16], [1], [2], [15], [3], [264], [800], [103], [9851], [1254], [392], [170], [495], [533], [9314], [103, 10, 62], [416], [232, 10, 240], [946], [3616], [346], [88], [1], [1471], [2], [54], [100], [264], [1672], [2114], [18186], [1], [2], [8], [492], [40041], [18186], [1], [464], [1471], [2], [420], [2225], [], [10, 62], [], [3172], [], [10, 240], [42], [4990], [88], [125], [1147], [831], [181], [21], [18186], [933], [1], [320], [2828], [2], [420], [264], [870], [323], [19312], [1], [627], [323], [231], [2828], [866], [4171], [59], [264], [1500], [6232], [70], [1272], [18186], [533], [110], [2771], [416], [2660], [346], [88], [684], [191], [1250], [1032], [1680], [18186], [533], [2771], [2377, 10, 7], [2658, 10, 26], [5498, 10, 62], [416], [7682, 10, 107], [14352], [2771], [1121], [103], [250], [1557], [4091], [866], [1]]\n",
      "[[2]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \n",
      "[[551], [85], [1], [132], [1], [557], [2], [209], [42], [2806], [1393], [466], [419], [2], [5], [233], [181], [1393], [466], [402, 1066], [16, 1631, 1066]]\n",
      "[[551]\n",
      " [  0]\n",
      " [  0]\n",
      " ...\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Questions from the website (might be in the dataset, need to check)\n",
    "# HQ score       = 25042, ID = 11227809\n",
    "# LQ_CLOSE score =   -26, ID = 24681866\n",
    "# LQ_EDIT score  =     9, ID =  3977535\n",
    "hq_question = \"\"\"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\"\"\"\n",
    "lq_close_question = \"\"\"My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \"\"\"\n",
    "lq_edit_question = \"\"\"I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\"\"\"\n",
    "questions = [hq_question, lq_edit_question, lq_close_question]\n",
    "\n",
    "for i in questions:\n",
    "    print(i)\n",
    "    sequenced = token.texts_to_sequences(i.split())\n",
    "    print(sequenced)\n",
    "    padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "    print(padded.reshape(-1,1))\n",
    "    prediction = mnb.predict(padded)\n",
    "    # Padded computes but results don't match (it converts the sequence (nested list) into a 2d array but )\n",
    "    print(prediction, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb_model = mnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pred = mnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.42      0.16      0.23     16037\n",
      "           1       0.56      0.22      0.31     16023\n",
      "           0       0.39      0.88      0.54     15940\n",
      "\n",
      "    accuracy                           0.42     48000\n",
      "   macro avg       0.46      0.42      0.36     48000\n",
      "weighted avg       0.46      0.42      0.36     48000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial Naive Bayes Classification Report\", \" \",classification_report(Y_train, mnb_pred, labels= [2, 1, 0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising Alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb_params = {'alpha' : np.linspace(0, 1, num=20)}\n",
    "\n",
    "# grid = GridSearchCV(estimator = MultinomialNB(), \n",
    "#                     param_grid = mnb_params, \n",
    "#                     scoring = 'accuracy',\n",
    "#                     cv = KFold(n_splits=10,\n",
    "#                               random_state=42,\n",
    "#                               shuffle=True\n",
    "#                              ),\n",
    "#                     n_jobs = -1,\n",
    "#                     return_train_score=True\n",
    "#                    )\n",
    "\n",
    "# grid = grid.fit(X_train, nb_target)\n",
    "# alpha = grid.best_params_['alpha']\n",
    "# print(f'alpha: {round(alpha,5)}')\n",
    "# print(f'Accuracy: {round(grid.best_score_,5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier\n",
    "Scaling is necessary for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_s = MinMaxScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg = SGDClassifier(loss='log', \n",
    "                    penalty='elasticnet', \n",
    "                    alpha=0.0001, \n",
    "                    l1_ratio=0.15, \n",
    "                    fit_intercept=True, \n",
    "                    max_iter=1000, \n",
    "                    tol=0.001, \n",
    "                    shuffle=True, \n",
    "                    verbose=0, \n",
    "                    epsilon=0.1, \n",
    "                    n_jobs=-1, \n",
    "                    random_state=42, \n",
    "                    learning_rate='optimal', \n",
    "                    eta0=0.0, \n",
    "                    power_t=0.5, \n",
    "                    early_stopping=True, \n",
    "                    validation_fraction=0.1, \n",
    "                    n_iter_no_change=5, \n",
    "                    class_weight=None, \n",
    "                    warm_start=False, \n",
    "                    average=False\n",
    "                   )\n",
    "sdg_model = sdg.fit(X_train_s, Y_train)\n",
    "sdg_pred = sdg.predict(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.45      0.22      0.29     16037\n",
      "           1       0.56      0.34      0.42     16023\n",
      "           0       0.42      0.80      0.55     15940\n",
      "\n",
      "    accuracy                           0.45     48000\n",
      "   macro avg       0.47      0.45      0.42     48000\n",
      "weighted avg       0.47      0.45      0.42     48000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine Classification Report\", \" \",classification_report(Y_train, sdg_pred, labels= [2, 1, 0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree = DecisionTreeClassifier(max_depth = 2)\n",
    "dtree_model = dtree.fit(X_train, Y_train) \n",
    "dtree_pred = dtree_model.predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.42      0.50      0.46     16037\n",
      "           1       0.59      0.20      0.30     16023\n",
      "           0       0.46      0.67      0.54     15940\n",
      "\n",
      "    accuracy                           0.46     48000\n",
      "   macro avg       0.49      0.46      0.43     48000\n",
      "weighted avg       0.49      0.46      0.43     48000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Classification Report\", \" \",classification_report(Y_train, dtree_pred, labels= [2, 1, 0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Padded Train Test Split\n",
    "#### X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "# This number could be tweaked (look at histogram of word count per class)\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(padded_X.shape, ' ', padded_X, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['Y']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y.shape,' ', y, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size+1, max_words_in_post, input_length=X_train.shape[1], mask_zero=True))\n",
    "# model.add(SpatialDropout1D(0.3))\n",
    "# model.add(GRU(75, dropout=0.2))\n",
    "# model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam', \n",
    "#               metrics=['categorical_accuracy']\n",
    "#              )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(loaded, to_file='data/img/BodyOnlyModel.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model (on Google Colab)\n",
    "\n",
    "I have exported the training step to a google colab gpu runtime to speed up the process. I have tried several sequential neural networks:\n",
    "* Embedding - SpatialDropout1D - LSTM - Dense\n",
    "* Embedding - SpatialDropout1D - LSTM - SpatialDropout1D - LSTM - Dense\n",
    "* Embedding - SpatialDropout1D - GRU - Dense\n",
    "\n",
    "The fitting parameters stayed the same for all models, but the layers parameters changed between the iterations. A couple of layers did not see much variation:\n",
    "* The embedding layer (input layer) with an input size of 73748 (the vocabulary +1), an output size of 100 or 150 (maximum words in a question) and the `zero_mask=` flag on (the training stage is faster with this option 'on'; from about 8 epochs to about 4 for the same model parameters)\n",
    "* The final dense layer (output layer) used a \"softmax\" activation, and had 3 nodes (for the 3 grades of questions).\n",
    "\n",
    "The best model (in terms of categorical_accuracy) is the GRU, reaching 0.9179. The LSTM is right behind with a categorical_accuracy of 0.9143. The training times are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train,\n",
    "#                     Y_train,\n",
    "#                     epochs=20,\n",
    "#                     validation_split=0.1,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss',\n",
    "#                                              min_delta=0.0001,\n",
    "#                                              patience=3,\n",
    "#                                              verbose=1,\n",
    "#                                              mode='min',\n",
    "#                                              restore_best_weights=True\n",
    "#                                              ),\n",
    "#                             #    ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Model/Checkpoints/epoch-{epoch:03d}.ckpt', \n",
    "#                             #                    verbose=1\n",
    "#                             #                    )\n",
    "#                               ]\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='val')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['accuracy'], label='train')\n",
    "# plt.plot(history.history['val_accuracy'], label='val')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.save('data/model//')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded = keras.models.load_model('data/model/embGRU/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = loaded.evaluate(X_test,Y_test)\n",
    "# print(f'Test set:\\n\\\n",
    "#           Loss: {loss:0.3f}\\n\\\n",
    "#           Accuracy: {accuracy:0.3f}'\n",
    "#      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Questions from the website (might be in the dataset, need to check)\n",
    "# # HQ score = 25042, ID = 11227809\n",
    "# # LQ_CLOSE = -26, ID = 24681866\n",
    "# # LQ_EDIT = 9, ID = 3977535\n",
    "# hq_question = \"\"\"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\"\"\"\n",
    "# lq_close_question = \"\"\"My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \"\"\"\n",
    "# lq_edit_question = \"\"\"I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\"\"\"\n",
    "# questions = [hq_question, lq_edit_question, lq_close_question]\n",
    "\n",
    "# for i in questions:\n",
    "#     sequenced = token.texts_to_sequences(i.split())\n",
    "#     padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "#     prediction = loaded.predict(padded)\n",
    "#     labels = pd.get_dummies(df['Y']).columns\n",
    "#     print(prediction[1], labels[np.argmax(prediction[1])], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df.sample(5).index:\n",
    "#     sequenced = token.texts_to_sequences(df['Body'].loc[i].split())\n",
    "#     padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "#     prediction = loaded.predict(padded)\n",
    "#     labels = pd.get_dummies(df['Y']).columns\n",
    "#     print(f\"Predicted:{labels[np.argmax(prediction[1])]}\\n     Real:{df['Y'].loc[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have an accuracy of about 75%. Unfortunately, the prediction chapter shows that mis-classifications are still fairly common. We can marginally improve this score by not restricting the length of vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Vocabulary Limitation\n",
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = keras.models.load_model('data/model/embGRU-AllVoc/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = loaded.evaluate(X_test,Y_test)\n",
    "print(f'Test set:\\n\\\n",
    "          Loss: {loss:0.3f}\\n\\\n",
    "          Accuracy: {accuracy:0.3f}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not in line with google colab accuracy. Have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
