{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "# from tensorflow.keras.layers import GRU\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import MaxPool1D\n",
    "# from tensorflow.keras.layers import SpatialDropout1D\n",
    "# from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/data_with_stem_lem.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('code', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Padded Train Test Split\n",
    "### X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 73_747 # From 03-WordAnalysis.ipynb\n",
    "# vocab_size = 290_479 # No restriction on vocab length\n",
    "token = Tokenizer(num_words=vocab_size,\n",
    "                  filters=\"\"\"!\"#$'%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\",\n",
    "                  lower=True\n",
    "                 )\n",
    "\n",
    "token.fit_on_texts(df['Body'].values)\n",
    "X = token.texts_to_sequences(df['Body'].values)\n",
    "len(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "# This number could be tweaked (look at histogram of word count per class)\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 150)\n",
      " \n",
      "[[   2    6  228 ...    0    0    0]\n",
      " [ 110  664    3 ...    0    0    0]\n",
      " [   2  113    3 ...    0    0    0]\n",
      " ...\n",
      " [1731  507    1 ...    0    0    0]\n",
      " [ 488  184   87 ...    0    0    0]\n",
      " [ 744 2490  186 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_X.shape, ' ', padded_X, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Y'].replace({'HQ':2,'LQ_EDIT':1,'LQ_CLOSE':0}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,\n",
    "                                                    Y_train,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38400, 150) (38400,)\n",
      "(9600, 150) (9600,)\n",
      "(12000, 150) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb_model = gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_pred = gnb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.93      0.54      3206\n",
      "           1       0.63      0.24      0.34      3129\n",
      "           2       0.43      0.08      0.13      3265\n",
      "\n",
      "    accuracy                           0.42      9600\n",
      "   macro avg       0.48      0.42      0.34      9600\n",
      "weighted avg       0.48      0.42      0.34      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gaussian Naive Bayes Classification Report\", \" \",classification_report(Y_val, gnb_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb_model = mnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pred = mnb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.88      0.55      3206\n",
      "           1       0.53      0.22      0.32      3129\n",
      "           2       0.42      0.15      0.23      3265\n",
      "\n",
      "    accuracy                           0.42      9600\n",
      "   macro avg       0.45      0.42      0.36      9600\n",
      "weighted avg       0.45      0.42      0.36      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial Naive Bayes Classification Report\", \" \",classification_report(Y_val, mnb_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising Alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb_params = {'alpha' : np.linspace(0, 1, num=20)}\n",
    "\n",
    "# grid = GridSearchCV(estimator = MultinomialNB(), \n",
    "#                     param_grid = mnb_params, \n",
    "#                     scoring = 'accuracy',\n",
    "#                     cv = KFold(n_splits=10,\n",
    "#                               random_state=42,\n",
    "#                               shuffle=True\n",
    "#                              ),\n",
    "#                     n_jobs = -1,\n",
    "#                     return_train_score=True\n",
    "#                    )\n",
    "\n",
    "# grid = grid.fit(X_train, nb_target)\n",
    "# alpha = grid.best_params_['alpha']\n",
    "# print(f'alpha: {round(alpha,5)}')\n",
    "# print(f'Accuracy: {round(grid.best_score_,5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier\n",
    "Scaling is necessary for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_s = MinMaxScaler().fit_transform(X_train)\n",
    "X_val_s = MinMaxScaler().fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='log', \n",
    "                    penalty='elasticnet', \n",
    "                    alpha=0.0001, \n",
    "                    l1_ratio=0.15, \n",
    "                    fit_intercept=True, \n",
    "                    max_iter=1000, \n",
    "                    tol=0.001, \n",
    "                    shuffle=True, \n",
    "                    verbose=0, \n",
    "                    epsilon=0.1, \n",
    "                    n_jobs=-1, \n",
    "                    random_state=42, \n",
    "                    learning_rate='optimal', \n",
    "                    eta0=0.0, \n",
    "                    power_t=0.5, \n",
    "                    early_stopping=True, \n",
    "                    validation_fraction=0.1, \n",
    "                    n_iter_no_change=5, \n",
    "                    class_weight=None, \n",
    "                    warm_start=False, \n",
    "                    average=False\n",
    "                   )\n",
    "sgd_model = sgd.fit(X_train_s, Y_train)\n",
    "sgd_pred = sgd.predict(X_val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.42      0.12      0.18      3265\n",
      "           1       0.50      0.38      0.43      3129\n",
      "           0       0.41      0.81      0.55      3206\n",
      "\n",
      "    accuracy                           0.43      9600\n",
      "   macro avg       0.44      0.44      0.39      9600\n",
      "weighted avg       0.44      0.43      0.39      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine Classification Report\", \" \",classification_report(Y_val, sdg_pred, labels= [2, 1, 0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_model = dtree.fit(X_train, Y_train) \n",
    "dtree_pred = dtree_model.predict(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.41      0.41      0.41      3265\n",
      "           1       0.50      0.51      0.50      3129\n",
      "           0       0.44      0.43      0.44      3206\n",
      "\n",
      "    accuracy                           0.45      9600\n",
      "   macro avg       0.45      0.45      0.45      9600\n",
      "weighted avg       0.45      0.45      0.45      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Classification Report\", \" \",classification_report(Y_val, dtree_pred, labels= [2, 1, 0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "8480\n"
     ]
    }
   ],
   "source": [
    "print(dtree.get_depth(), dtree.get_n_leaves(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output\n",
      "HQ\n",
      "LQ_EDIT\n",
      "LQ_CLOSE\n",
      "----------\n",
      "Prediction for GaussianNB()\n",
      "HQ\n",
      "LQ_EDIT\n",
      "LQ_CLOSE\n",
      "----------\n",
      "Prediction for MultinomialNB(alpha=1)\n",
      "LQ_EDIT\n",
      "LQ_EDIT\n",
      "LQ_CLOSE\n",
      "----------\n",
      "Prediction for SGDClassifier(early_stopping=True, loss='log', n_jobs=-1, penalty='elasticnet',\n",
      "              random_state=42)\n",
      "LQ_EDIT\n",
      "LQ_EDIT\n",
      "LQ_EDIT\n",
      "----------\n",
      "Prediction for DecisionTreeClassifier()\n",
      "HQ\n",
      "LQ_CLOSE\n",
      "LQ_EDIT\n"
     ]
    }
   ],
   "source": [
    "models = [gnb, mnb, sgd, dtree]\n",
    "\n",
    "# Questions from the website (might be in the dataset, need to check)\n",
    "# HQ score       = 25042, ID = 11227809\n",
    "# LQ_CLOSE score =   -26, ID = 24681866\n",
    "# LQ_EDIT score  =     9, ID =  3977535\n",
    "hq_question = \"\"\"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\"\"\"\n",
    "lq_close_question = \"\"\"My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \"\"\"\n",
    "lq_edit_question = \"\"\"I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\"\"\"\n",
    "questions = [hq_question, lq_edit_question, lq_close_question]\n",
    "\n",
    "sequenced = token.texts_to_sequences(questions)\n",
    "padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "print('Expected Output\\nHQ\\nLQ_EDIT\\nLQ_CLOSE')\n",
    "for model in models:\n",
    "    prediction = model.predict(padded)\n",
    "    print(10*'-',f\"Prediction for {model}\", sep='\\n')\n",
    "    for i in prediction:\n",
    "        if i == 2:\n",
    "            print('HQ')\n",
    "        elif i == 1:\n",
    "            print('LQ_EDIT')\n",
    "        else:\n",
    "            print('LQ_CLOSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Padded Train Test Split\n",
    "#### X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "# This number could be tweaked (look at histogram of word count per class)\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(padded_X.shape, ' ', padded_X, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['Y']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y.shape,' ', y, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size+1, max_words_in_post, input_length=X_train.shape[1], mask_zero=True))\n",
    "# model.add(SpatialDropout1D(0.3))\n",
    "# model.add(GRU(75, dropout=0.2))\n",
    "# model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam', \n",
    "#               metrics=['categorical_accuracy']\n",
    "#              )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(loaded, to_file='data/img/BodyOnlyModel.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model (on Google Colab)\n",
    "\n",
    "I have exported the training step to a google colab gpu runtime to speed up the process. I have tried several sequential neural networks:\n",
    "* Embedding - SpatialDropout1D - LSTM - Dense\n",
    "* Embedding - SpatialDropout1D - LSTM - SpatialDropout1D - LSTM - Dense\n",
    "* Embedding - SpatialDropout1D - GRU - Dense\n",
    "\n",
    "The fitting parameters stayed the same for all models, but the layers parameters changed between the iterations. A couple of layers did not see much variation:\n",
    "* The embedding layer (input layer) with an input size of 73748 (the vocabulary +1), an output size of 100 or 150 (maximum words in a question) and the `zero_mask=` flag on (the training stage is faster with this option 'on'; from about 8 epochs to about 4 for the same model parameters)\n",
    "* The final dense layer (output layer) used a \"softmax\" activation, and had 3 nodes (for the 3 grades of questions).\n",
    "\n",
    "The best model (in terms of categorical_accuracy) is the GRU, reaching 0.9179. The LSTM is right behind with a categorical_accuracy of 0.9143. The training times are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train,\n",
    "#                     Y_train,\n",
    "#                     epochs=20,\n",
    "#                     validation_split=0.1,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss',\n",
    "#                                              min_delta=0.0001,\n",
    "#                                              patience=3,\n",
    "#                                              verbose=1,\n",
    "#                                              mode='min',\n",
    "#                                              restore_best_weights=True\n",
    "#                                              ),\n",
    "#                             #    ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Model/Checkpoints/epoch-{epoch:03d}.ckpt', \n",
    "#                             #                    verbose=1\n",
    "#                             #                    )\n",
    "#                               ]\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='val')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['accuracy'], label='train')\n",
    "# plt.plot(history.history['val_accuracy'], label='val')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.save('data/model//')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = keras.models.load_model('data/model/embGRU/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = loaded.evaluate(X_test,Y_test)\n",
    "# print(f'Test set:\\n\\\n",
    "#           Loss: {loss:0.3f}\\n\\\n",
    "#           Accuracy: {accuracy:0.3f}'\n",
    "#      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0021616  0.45251447 0.54532397]\n",
      "LQ_EDIT\n",
      "[0.8673554  0.09242888 0.04021572]\n",
      "HQ\n",
      "[0.26463172 0.60632634 0.12904194]\n",
      "LQ_CLOSE\n"
     ]
    }
   ],
   "source": [
    "# Questions from the website (might be in the dataset, need to check)\n",
    "# HQ score = 25042, ID = 11227809\n",
    "# LQ_CLOSE = -26, ID = 24681866\n",
    "# LQ_EDIT = 9, ID = 3977535\n",
    "hq_question = \"\"\"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\"\"\"\n",
    "lq_close_question = \"\"\"My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \"\"\"\n",
    "lq_edit_question = \"\"\"I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\"\"\"\n",
    "questions = [hq_question, lq_edit_question, lq_close_question]\n",
    "\n",
    "sequenced = token.texts_to_sequences(questions)\n",
    "padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "prediction = loaded.predict(padded)\n",
    "labels = pd.get_dummies(df['Y']).columns\n",
    "for i in prediction:\n",
    "    print(i, labels[np.argmax(i)], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have an accuracy of about 75%. Unfortunately, the prediction chapter shows that mis-classifications are still fairly common. We can marginally improve this score by not restricting the length of vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Vocabulary Limitation\n",
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = keras.models.load_model('data/model/embGRU-AllVoc/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = loaded.evaluate(X_test,Y_test)\n",
    "print(f'Test set:\\n\\\n",
    "          Loss: {loss:0.3f}\\n\\\n",
    "          Accuracy: {accuracy:0.3f}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not in line with google colab accuracy. Have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
