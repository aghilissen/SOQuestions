{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, MaxPool1D, SpatialDropout1D\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/data_with_stem_lem.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('code', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Train Test Split\n",
    "### X\n",
    "The vocabulary size and the cut-off for a question length have been determined in the third notebook: [Word Analysis](./03-WordAnalysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 73_747\n",
    "token = Tokenizer(num_words=vocab_size,\n",
    "                  filters=\"\"\"!\"#$'%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\",\n",
    "                  lower=True\n",
    "                 )\n",
    "\n",
    "token.fit_on_texts(df['Body'].values)\n",
    "X = token.texts_to_sequences(df['Body'].values)\n",
    "len(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Y'].replace({'HQ':2,'LQ_EDIT':1,'LQ_CLOSE':0}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,\n",
    "                                                    Y_train,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38400, 150) (38400,)\n",
      "(9600, 150) (9600,)\n",
      "(12000, 150) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb_model = gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_pred = gnb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.93      0.54      3206\n",
      "           1       0.63      0.24      0.34      3129\n",
      "           2       0.43      0.08      0.13      3265\n",
      "\n",
      "    accuracy                           0.42      9600\n",
      "   macro avg       0.48      0.42      0.34      9600\n",
      "weighted avg       0.48      0.42      0.34      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gaussian Naive Bayes Classification Report\",\n",
    "      \" \",\n",
    "      classification_report(Y_val,\n",
    "                            gnb_pred\n",
    "                           ),\n",
    "      sep='\\n'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb_model = mnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pred = mnb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.88      0.55      3206\n",
      "           1       0.53      0.22      0.32      3129\n",
      "           2       0.42      0.15      0.23      3265\n",
      "\n",
      "    accuracy                           0.42      9600\n",
      "   macro avg       0.45      0.42      0.36      9600\n",
      "weighted avg       0.45      0.42      0.36      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial Naive Bayes Classification Report\",\n",
    "      \" \",\n",
    "      classification_report(Y_val,\n",
    "                            mnb_pred\n",
    "                           ),\n",
    "      sep='\\n'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimising Alpha:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb_params = {'alpha' : np.linspace(0, 1, num=20)}\n",
    "\n",
    "# grid = GridSearchCV(estimator = MultinomialNB(), \n",
    "#                     param_grid = mnb_params, \n",
    "#                     scoring = 'accuracy',\n",
    "#                     cv = KFold(n_splits=10,\n",
    "#                               random_state=42,\n",
    "#                               shuffle=True\n",
    "#                              ),\n",
    "#                     n_jobs = -1,\n",
    "#                     return_train_score=True\n",
    "#                    )\n",
    "\n",
    "# grid = grid.fit(X_train, nb_target)\n",
    "# alpha = grid.best_params_['alpha']\n",
    "# print(f'alpha: {round(alpha,5)}')\n",
    "# print(f'Accuracy: {round(grid.best_score_,5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier\n",
    "Scaling is necessary for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_s = MinMaxScaler().fit_transform(X_train)\n",
    "X_val_s = MinMaxScaler().fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='log', \n",
    "                    penalty='elasticnet', \n",
    "                    alpha=0.0001, \n",
    "                    l1_ratio=0.15, \n",
    "                    fit_intercept=True, \n",
    "                    max_iter=1000, \n",
    "                    tol=0.001, \n",
    "                    shuffle=True, \n",
    "                    verbose=0, \n",
    "                    epsilon=0.1, \n",
    "                    n_jobs=-1, \n",
    "                    random_state=42, \n",
    "                    learning_rate='optimal', \n",
    "                    eta0=0.0, \n",
    "                    power_t=0.5, \n",
    "                    early_stopping=True, \n",
    "                    validation_fraction=0.1, \n",
    "                    n_iter_no_change=5, \n",
    "                    class_weight=None, \n",
    "                    warm_start=False, \n",
    "                    average=False\n",
    "                   )\n",
    "sgd_model = sgd.fit(X_train_s, Y_train)\n",
    "sgd_pred = sgd.predict(X_val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.42      0.12      0.18      3265\n",
      "           1       0.50      0.38      0.43      3129\n",
      "           0       0.41      0.81      0.55      3206\n",
      "\n",
      "    accuracy                           0.43      9600\n",
      "   macro avg       0.44      0.44      0.39      9600\n",
      "weighted avg       0.44      0.43      0.39      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine Classification Report\",\n",
    "      \" \",\n",
    "      classification_report(Y_val,\n",
    "                            sgd_pred,\n",
    "                            labels= [2, 1, 0]\n",
    "                           ),\n",
    "      sep='\\n'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree_model = dtree.fit(X_train, Y_train) \n",
    "dtree_pred = dtree_model.predict(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.41      0.41      0.41      3265\n",
      "           1       0.50      0.51      0.50      3129\n",
      "           0       0.45      0.44      0.45      3206\n",
      "\n",
      "    accuracy                           0.45      9600\n",
      "   macro avg       0.45      0.45      0.45      9600\n",
      "weighted avg       0.45      0.45      0.45      9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Classification Report\",\n",
    "      \" \",classification_report(Y_val,\n",
    "                                dtree_pred,\n",
    "                                labels= [2, 1, 0]\n",
    "                               ),\n",
    "      sep='\\n'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth:46, number of leaves:8493\n"
     ]
    }
   ],
   "source": [
    "print(f'Tree depth:{dtree.get_depth()}, number of leaves:{dtree.get_n_leaves()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Prediction for GaussianNB()\n",
      "HQ= 7.0%\n",
      "LQ_EDIT= 15.045%\n",
      "LQ_CLOSE= 77.955%\n",
      "----------\n",
      "Prediction for MultinomialNB(alpha=1)\n",
      "HQ= 15.69%\n",
      "LQ_EDIT= 14.2%\n",
      "LQ_CLOSE= 70.11%\n",
      "----------\n",
      "Prediction for SGDClassifier(early_stopping=True, loss='log', n_jobs=-1, penalty='elasticnet',\n",
      "              random_state=42)\n",
      "HQ= 1.045%\n",
      "LQ_EDIT= 98.825%\n",
      "LQ_CLOSE= 0.13%\n",
      "----------\n",
      "Prediction for DecisionTreeClassifier()\n",
      "HQ= 74.66%\n",
      "LQ_EDIT= 11.665%\n",
      "LQ_CLOSE= 13.675%\n"
     ]
    }
   ],
   "source": [
    "models = [gnb, mnb, sgd, dtree]\n",
    "sequenced = token.texts_to_sequences(df[df['Y']=='HQ']['Body'].values)\n",
    "padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "for model in models:\n",
    "    prediction = model.predict(padded)\n",
    "    print(10*'-',f\"Prediction for {model}\", sep='\\n')\n",
    "    HQ = 0\n",
    "    LQ_EDIT = 0\n",
    "    LQ_CLOSE = 0\n",
    "    for i in prediction:\n",
    "        if i == 2:\n",
    "            HQ +=1\n",
    "        elif i == 1:\n",
    "            LQ_EDIT += 1\n",
    "        else:\n",
    "            LQ_CLOSE += 1\n",
    "    print(f'HQ= {HQ/200}%\\nLQ_EDIT= {LQ_EDIT/200}%\\nLQ_CLOSE= {LQ_CLOSE/200}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output\n",
      "HQ\n",
      "LQ_EDIT\n",
      "LQ_CLOSE\n",
      "----------\n",
      "Prediction for GaussianNB()\n",
      "HQ\n",
      "LQ_EDIT\n",
      "LQ_CLOSE\n",
      "----------\n",
      "Prediction for MultinomialNB(alpha=1)\n",
      "LQ_EDIT\n",
      "LQ_EDIT\n",
      "LQ_CLOSE\n",
      "----------\n",
      "Prediction for SGDClassifier(early_stopping=True, loss='log', n_jobs=-1, penalty='elasticnet',\n",
      "              random_state=42)\n",
      "LQ_EDIT\n",
      "LQ_EDIT\n",
      "LQ_EDIT\n",
      "----------\n",
      "Prediction for DecisionTreeClassifier()\n",
      "HQ\n",
      "LQ_EDIT\n",
      "HQ\n"
     ]
    }
   ],
   "source": [
    "models = [gnb, mnb, sgd, dtree]\n",
    "\n",
    "# Questions from the website (might be in the dataset, need to check)\n",
    "# HQ score       = 25042, ID = 11227809\n",
    "# LQ_CLOSE score =   -26, ID = 24681866\n",
    "# LQ_EDIT score  =     9, ID =  3977535\n",
    "hq_question = \"\"\"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\"\"\"\n",
    "lq_close_question = \"\"\"My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \"\"\"\n",
    "lq_edit_question = \"\"\"I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\"\"\"\n",
    "questions = [hq_question, lq_edit_question, lq_close_question]\n",
    "\n",
    "sequenced = token.texts_to_sequences(questions)\n",
    "padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "print('Expected Output\\nHQ\\nLQ_EDIT\\nLQ_CLOSE')\n",
    "for model in models:\n",
    "    prediction = model.predict(padded)\n",
    "    print(10*'-',f\"Prediction for {model}\", sep='\\n')\n",
    "    for i in prediction:\n",
    "        if i == 2:\n",
    "            print('HQ')\n",
    "        elif i == 1:\n",
    "            print('LQ_EDIT')\n",
    "        else:\n",
    "            print('LQ_CLOSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Padded Train Test Split\n",
    "#### X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "# This number could be tweaked (look at histogram of word count per class)\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 150)\n",
      " \n",
      "[[   2    6  228 ...    0    0    0]\n",
      " [ 110  664    3 ...    0    0    0]\n",
      " [   2  113    3 ...    0    0    0]\n",
      " ...\n",
      " [1731  507    1 ...    0    0    0]\n",
      " [ 488  184   87 ...    0    0    0]\n",
      " [ 744 2490  186 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_X.shape, ' ', padded_X, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['Y']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3)\n",
      " \n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y.shape,' ', y, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 150) (48000, 3)\n",
      "(12000, 150) (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size+1, max_words_in_post, input_length=X_train.shape[1], mask_zero=True))\n",
    "# model.add(SpatialDropout1D(0.3))\n",
    "# model.add(GRU(75, dropout=0.2))\n",
    "# model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam', \n",
    "#               metrics=['categorical_accuracy']\n",
    "#              )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(loaded, to_file='data/img/BodyOnlyModel.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model (on Google Colab)\n",
    "\n",
    "I have exported the training step to a google colab gpu runtime to speed up the process. I have tried several sequential neural networks:\n",
    "* Embedding - SpatialDropout1D - LSTM - Dense\n",
    "* Embedding - SpatialDropout1D - LSTM - SpatialDropout1D - LSTM - Dense\n",
    "* Embedding - SpatialDropout1D - GRU - Dense\n",
    "\n",
    "The fitting parameters stayed the same for all models, but the layers parameters changed between the iterations. A couple of layers did not see much variation:\n",
    "* The embedding layer (input layer) with an input size of 73748 (the vocabulary +1), an output size of 100 or 150 (maximum words in a question) and the `zero_mask=` flag on (the training stage is faster with this option 'on'; from about 8 epochs to about 4 for the same model parameters)\n",
    "* The final dense layer (output layer) used a \"softmax\" activation, and had 3 nodes (for the 3 grades of questions).\n",
    "\n",
    "The best model (in terms of categorical_accuracy) is the GRU, reaching 0.9179. The LSTM is right behind with a categorical_accuracy of 0.9143. The training times are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train,\n",
    "#                     Y_train,\n",
    "#                     epochs=20,\n",
    "#                     validation_split=0.1,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss',\n",
    "#                                              min_delta=0.0001,\n",
    "#                                              patience=3,\n",
    "#                                              verbose=1,\n",
    "#                                              mode='min',\n",
    "#                                              restore_best_weights=True\n",
    "#                                              ),\n",
    "#                             #    ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Model/Checkpoints/epoch-{epoch:03d}.ckpt', \n",
    "#                             #                    verbose=1\n",
    "#                             #                    )\n",
    "#                               ]\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='val')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['accuracy'], label='train')\n",
    "# plt.plot(history.history['val_accuracy'], label='val')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.save('data/model//')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_model('data/model/emb-lstmx2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = loaded.evaluate(X_test,Y_test)\n",
    "print(f'Test set:\\n\\\n",
    "          Loss: {loss:0.3f}\\n\\\n",
    "          Accuracy: {accuracy:0.3f}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0021616  0.45251447 0.54532397]\n",
      "LQ_EDIT\n",
      "[0.8673554  0.09242888 0.04021572]\n",
      "HQ\n",
      "[0.26463172 0.60632634 0.12904194]\n",
      "LQ_CLOSE\n"
     ]
    }
   ],
   "source": [
    "# Questions from the website (might be in the dataset, need to check)\n",
    "# HQ score = 25042, ID = 11227809\n",
    "# LQ_CLOSE = -26, ID = 24681866\n",
    "# LQ_EDIT = 9, ID = 3977535\n",
    "hq_question = \"\"\"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: code Without std::sort(data, data + arraySize);, the code runs in 11.54 seconds. With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: code  With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.\"\"\"\n",
    "lq_close_question = \"\"\"My html code is code How I convert it into wordpress menu? Actually I want include in wordpress menu title=\"features\" data-hover=\"Features\" \"\"\"\n",
    "lq_edit_question = \"\"\"I have two different timeseries with partially overlapping timestamps: code which represents following data: code I would like to calculate a weighted average on every day with coefficients a(0.3) and b(0.7), while ignoring missing values: code when I first try to align these timeseries: code I get correctly masked timeseries: code but when I do a1 * 0.3 + b1 * 0.7, it ignores values, that are present in one timeseries only: code What should I do to receive the awaited? code EDIT: The answer should be applicable also to more than two initial timeseries with different weights and differently missing values. o if we have four timeseries with weights T1(0.1), T2(0.2), T3(0.3) and T4(0.4), their weights at a given timestamp will be: code\"\"\"\n",
    "questions = [hq_question, lq_edit_question, lq_close_question]\n",
    "\n",
    "sequenced = token.texts_to_sequences(questions)\n",
    "padded = pad_sequences(sequenced, maxlen=max_words_in_post, padding='post')\n",
    "prediction = loaded.predict(padded)\n",
    "labels = pd.get_dummies(df['Y']).columns\n",
    "for i in prediction:\n",
    "    print(i, labels[np.argmax(i)], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have an accuracy of about 75%. Unfortunately, the prediction chapter shows that mis-classifications are still fairly common. We can marginally improve this score by not restricting the length of vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Vocabulary Limitation\n",
    "## Train Test Split\n",
    "### X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105215"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 290_479 # No restriction on vocab length\n",
    "token = Tokenizer(num_words=vocab_size,\n",
    "                  filters=\"\"\"!\"#$'%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\",\n",
    "                  lower=True\n",
    "                 )\n",
    "\n",
    "token.fit_on_texts(df['Body'].values)\n",
    "X = token.texts_to_sequences(df['Body'].values)\n",
    "len(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_in_post = 150\n",
    "padded_X = pad_sequences(X, maxlen = max_words_in_post, padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['Y']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 150) (48000, 3)\n",
      "(9600, 150) (9600,)\n",
      "(12000, 150) (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_model('data/model/embGRU-AllVoc/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 7s 18ms/step - loss: 1.2727 - categorical_accuracy: 0.5002\n",
      "Test set:\n",
      "          Loss: 1.273\n",
      "          Accuracy: 0.500\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = loaded.evaluate(X_test,Y_test)\n",
    "print(f'Test set:\\n\\\n",
    "          Loss: {loss:0.3f}\\n\\\n",
    "          Accuracy: {accuracy:0.3f}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not in line with google colab accuracy. Have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
